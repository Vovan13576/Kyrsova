import os
import json
import argparse
import random
import math
import io
from pathlib import Path

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import numpy as np
import tensorflow as tf
from PIL import Image

from datasets import load_dataset


def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def find_parquet_files(root: Path):
    files = sorted([str(p) for p in root.rglob("*.parquet")])
    return files


def split_files_by_name(files):
    train = [f for f in files if "train" in Path(f).name.lower()]
    val = [f for f in files if "valid" in Path(f).name.lower() or "validation" in Path(f).name.lower()]
    test = [f for f in files if "test" in Path(f).name.lower()]

    # —è–∫—â–æ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–∑–≤–∞–≤ —Ñ–∞–π–ª–∏ —è–∫ train/val/test ‚Äî –ø–æ–≤–µ—Ä–Ω–µ–º–æ –≤—Å–µ –≤ train —ñ —Ä–æ–∑—ñ–±‚Äô—î–º–æ –≤–∂–µ –ø—ñ—Å–ª—è load_dataset
    if len(train) == 0 and len(val) == 0 and len(test) == 0:
        return {"all": files}

    out = {}
    if train: out["train"] = train
    if val: out["validation"] = val
    if test: out["test"] = test
    return out


def decode_any_image(x):
    """
    –ü—ñ–¥—Ç—Ä–∏–º—É—î —Ñ–æ—Ä–º–∞—Ç–∏, —è–∫—ñ —á–∞—Å—Ç–æ —Ç—Ä–∞–ø–ª—è—é—Ç—å—Å—è –≤ HF parquet:
    - PIL.Image
    - dict –∑ keys: {bytes, path}
    """
    if x is None:
        return None

    if isinstance(x, Image.Image):
        return x.convert("RGB")

    if isinstance(x, dict):
        b = x.get("bytes", None)
        p = x.get("path", None)
        if b is not None:
            return Image.open(io.BytesIO(b)).convert("RGB")
        if p:
            return Image.open(p).convert("RGB")

    # —ñ–Ω–∫–æ–ª–∏ –±—É–≤–∞—î –ø—Ä–æ—Å—Ç–æ —à–ª—è—Ö
    if isinstance(x, str) and Path(x).exists():
        return Image.open(x).convert("RGB")

    return None


def make_tf_dataset(hfds, image_col, label_col, img_size, batch_size, shuffle, max_samples, seed):
    n = len(hfds)
    if max_samples and max_samples > 0:
        n = min(n, max_samples)

    indices = list(range(n))
    if shuffle:
        rng = random.Random(seed)
        rng.shuffle(indices)

    def gen():
        for idx in indices:
            ex = hfds[idx]
            img = decode_any_image(ex[image_col])
            if img is None:
                continue
            img = img.resize((img_size, img_size))
            arr = np.asarray(img, dtype=np.float32) / 255.0
            y = ex[label_col]
            yield arr, y

    output_signature = (
        tf.TensorSpec(shape=(img_size, img_size, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(), dtype=tf.int32),
    )

    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)
    if shuffle:
        ds = ds.shuffle(buffer_size=min(10_000, n), seed=seed, reshuffle_each_iteration=True)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds, n


def build_model(num_classes, img_size, lr):
    base = tf.keras.applications.EfficientNetV2B0(
        include_top=False,
        weights="imagenet",
        input_shape=(img_size, img_size, 3),
    )
    base.trainable = False  # —Å—Ç–∞—Ä—Ç—É—î–º–æ —è–∫ transfer learning

    inputs = tf.keras.Input(shape=(img_size, img_size, 3))
    x = inputs
    x = tf.keras.layers.RandomFlip("horizontal")(x)
    x = tf.keras.layers.RandomRotation(0.06)(x)
    x = tf.keras.layers.RandomZoom(0.10)(x)
    x = base(x, training=False)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dropout(0.25)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation="softmax")(x)

    model = tf.keras.Model(inputs, outputs)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model, base


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="data/plantnet300k")
    ap.add_argument("--out_model", type=str, default="plantnet_model.keras")
    ap.add_argument("--out_labels", type=str, default="plantnet_labels.json")
    ap.add_argument("--img_size", type=int, default=224)
    ap.add_argument("--batch", type=int, default=32)
    ap.add_argument("--epochs", type=int, default=3)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--max_train", type=int, default=0, help="0 = –≤–µ—Å—å train; –¥–ª—è —Ç–µ—Å—Ç—É –ø–æ—Å—Ç–∞–≤ 20000")
    ap.add_argument("--max_val", type=int, default=0)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    set_seed(args.seed)

    root = Path(args.data_dir).resolve()
    if not root.exists():
        raise SystemExit(f"‚ùå –ù–µ–º–∞ –ø–∞–ø–∫–∏: {root}")

    parquet = find_parquet_files(root)
    if len(parquet) == 0:
        raise SystemExit("‚ùå –ù–µ –∑–Ω–∞–π—à–æ–≤ *.parquet —É data_dir. –ü–æ–∫–∞–∂–∏ –≤–º—ñ—Å—Ç –ø–∞–ø–∫–∏, —ñ —è –ø—ñ–¥–∫–∞–∂—É –ø—ñ–¥ —Ç–≤—ñ–π —Ñ–æ—Ä–º–∞—Ç.")

    groups = split_files_by_name(parquet)

    print("‚úÖ Parquet –∑–Ω–∞–π–¥–µ–Ω–æ:", len(parquet))
    print("‚úÖ –ì—Ä—É–ø–∏:", {k: len(v) for k, v in groups.items()})

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —è–∫ HF dataset –∑ –ª–æ–∫–∞–ª—å–Ω–∏—Ö parquet
    if "all" in groups:
        ds = load_dataset("parquet", data_files={"train": groups["all"]})
        # —Å–∞–º—ñ —Ä–æ–∑—ñ–±‚Äô—î–º–æ –Ω–∞ train/val
        split = ds["train"].train_test_split(test_size=0.12, seed=args.seed)
        train_hf = split["train"]
        val_hf = split["test"]
    else:
        ds = load_dataset("parquet", data_files=groups)
        train_hf = ds["train"]
        val_hf = ds["validation"] if "validation" in ds else ds["test"]

    # –ó–Ω–∞–π–¥–µ–º–æ –Ω–∞–∑–≤–∏ –∫–æ–ª–æ–Ω–æ–∫
    cols = train_hf.column_names
    image_col = "image" if "image" in cols else None
    label_col = "label" if "label" in cols else None
    if image_col is None or label_col is None:
        raise SystemExit(f"‚ùå –ù–µ –±–∞—á—É –∫–æ–ª–æ–Ω–æ–∫ image/label. –Ñ —Ç—ñ–ª—å–∫–∏: {cols}. –°–∫–∏–Ω—å –º–µ–Ω—ñ —Ü—ñ –∫–æ–ª–æ–Ω–∫–∏ ‚Äî —è –ø—ñ–¥–ª–∞—à—Ç—É—é –∫–æ–¥.")

    # labels: –∑—Ä–æ–±–∏–º–æ map int->name —è–∫—â–æ —î class labels
    label_names = None
    try:
        feat = train_hf.features[label_col]
        if hasattr(feat, "names") and feat.names:
            label_names = list(feat.names)
    except Exception:
        pass

    if label_names is None:
        # —è–∫—â–æ label –ø—Ä–æ—Å—Ç–æ int –±–µ–∑ –Ω–∞–∑–≤ ‚Äî –Ω–∞–∑–≤–µ–º–æ —è–∫ class_0...
        max_label = int(max(train_hf[label_col][:10000]))  # –≥—Ä—É–±–æ, –∞–ª–µ –æ–∫ –¥–ª—è —Å—Ç–∞—Ä—Ç—É
        label_names = [f"class_{i}" for i in range(max_label + 1)]

    num_classes = len(label_names)
    print("‚úÖ –ö–ª–∞—Å—ñ–≤:", num_classes)

    # TF datasets
    train_tf, train_n = make_tf_dataset(
        train_hf, image_col, label_col, args.img_size, args.batch,
        shuffle=True, max_samples=args.max_train, seed=args.seed
    )
    val_tf, val_n = make_tf_dataset(
        val_hf, image_col, label_col, args.img_size, args.batch,
        shuffle=False, max_samples=args.max_val, seed=args.seed
    )

    print(f"‚úÖ Train –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: {train_n}, Val –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: {val_n}")

    model, base = build_model(num_classes, args.img_size, args.lr)

    steps_per_epoch = max(1, math.floor(train_n / args.batch))
    val_steps = max(1, math.floor(val_n / args.batch))

    ckpt = tf.keras.callbacks.ModelCheckpoint(
        filepath="plantnet_best.keras",
        monitor="val_accuracy",
        save_best_only=True,
        verbose=1,
    )
    early = tf.keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=2,
        restore_best_weights=True,
        verbose=1,
    )

    print("üöÄ –°—Ç–∞—Ä—Ç —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (transfer learning)...")
    model.fit(
        train_tf,
        validation_data=val_tf,
        epochs=args.epochs,
        steps_per_epoch=steps_per_epoch,
        validation_steps=val_steps,
        callbacks=[ckpt, early],
        verbose=1,
    )

    # –ª–µ–≥–∫–∏–π finetune –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —à–∞—Ä—ñ–≤
    print("üõ†Ô∏è Finetune: —Ä–æ–∑–º–æ—Ä–æ–∂—É—é —á–∞—Å—Ç–∏–Ω—É EfficientNet...")
    base.trainable = True
    for layer in base.layers[:-40]:
        layer.trainable = False

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=args.lr * 0.1),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )

    model.fit(
        train_tf,
        validation_data=val_tf,
        epochs=max(1, args.epochs // 2),
        steps_per_epoch=steps_per_epoch,
        validation_steps=val_steps,
        verbose=1,
    )

    out_model = Path(args.out_model).resolve()
    out_labels = Path(args.out_labels).resolve()

    model.save(out_model)
    with open(out_labels, "w", encoding="utf-8") as f:
        json.dump({i: name for i, name in enumerate(label_names)}, f, ensure_ascii=False, indent=2)

    print("‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –º–æ–¥–µ–ª—å:", out_model)
    print("‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ labels:", out_labels)
    print("üéâ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
